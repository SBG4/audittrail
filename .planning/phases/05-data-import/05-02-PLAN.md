---
phase: 05-data-import
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - server/src/routers/imports.py
  - server/src/services/import_parser.py
autonomous: true

must_haves:
  truths:
    - "User can submit column mappings from spreadsheet columns to event fields"
    - "Each row is validated against event field constraints (date format, number type, etc.)"
    - "Validation response shows per-row success/failure with specific error messages"
    - "Invalid mappings (unknown event fields) are rejected"
  artifacts:
    - path: "server/src/routers/imports.py"
      provides: "Column mapping and validation endpoint"
    - path: "server/src/services/import_parser.py"
      provides: "Row validation logic with date/time/number parsing"
  key_links:
    - from: "server/src/routers/imports.py"
      to: "server/src/services/import_parser.py"
      via: "validate_and_transform_row() calls"
    - from: "server/src/routers/imports.py"
      to: "server/src/schemas/import_.py"
      via: "ColumnMapping and ImportValidationResponse schemas"
---

<objective>
Column mapping API with field validation and row-by-row error reporting.

Purpose: Allow the user to map spreadsheet columns to event fields and get detailed validation results before committing the import.
Output: Mapping validation endpoint and row transformation/validation logic.
</objective>

<execution_context>
@/Users/shiro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/shiro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-data-import/05-RESEARCH.md
@.planning/phases/05-data-import/05-01-SUMMARY.md
@server/src/routers/imports.py
@server/src/services/import_parser.py
@server/src/schemas/import_.py
@server/src/schemas/event.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add row validation logic to import parser</name>
  <files>
    server/src/services/import_parser.py
  </files>
  <action>
  Add validation/transformation functions to import_parser.py:

  1. `VALID_EVENT_FIELDS`: Set of valid event field names that can be mapped to:
     `{"event_type", "event_date", "event_time", "file_name", "file_count", "file_description", "file_type"}`

  2. `VALID_EVENT_TYPES`: Set `{"finding", "action", "note"}`

  3. `parse_date(value) -> date | None`:
     - Accept: date objects (return as-is), datetime objects (return .date()), strings
     - For strings, try these formats in order: `%Y-%m-%d`, `%m/%d/%Y`, `%d/%m/%Y`, `%Y/%m/%d`
     - Also try `dateutil.parser.parse` if simple formats fail (but avoid adding dateutil dep -- use multiple strptime patterns instead)
     - Return None if unparseable

  4. `parse_time(value) -> time | None`:
     - Accept: time objects (return as-is), datetime objects (return .time()), strings
     - For strings, try: `%H:%M:%S`, `%H:%M`, `%I:%M %p`, `%I:%M:%S %p`
     - Return None if unparseable

  5. `parse_int(value) -> int | None`:
     - Accept: int (return as-is), float (int if no decimal), str (try int(value.strip()))
     - Return None if unparseable

  6. `validate_and_transform_row(row_data: list, headers: list[str], mappings: dict[str, str]) -> tuple[bool, dict, list[str]]`:
     - For each mapping (spreadsheet_col -> event_field):
       - Find column index from headers
       - Get raw value from row_data
       - Apply appropriate parser based on event_field:
         - event_date -> parse_date (REQUIRED, error if None)
         - event_time -> parse_time (optional)
         - event_type -> validate against VALID_EVENT_TYPES (default "note" if empty)
         - file_count -> parse_int (optional)
         - file_name, file_description, file_type -> str(value).strip() (optional)
       - Collect errors per field
     - Return (is_valid, transformed_data_dict, error_messages)
     - event_date is the only required field. If not mapped or value is empty/unparseable, mark row as invalid.
  </action>
  <verify>
  Run: `cd /Users/shiro/projec1/server && python -c "
from src.services.import_parser import validate_and_transform_row, parse_date, parse_time
from datetime import date, time
assert parse_date('2024-01-15') == date(2024, 1, 15)
assert parse_time('14:30') == time(14, 30)
result = validate_and_transform_row(['2024-01-15', 'test.pdf', '5'], ['Date', 'File', 'Count'], {'Date': 'event_date', 'File': 'file_name', 'Count': 'file_count'})
print('Valid:', result[0], 'Data:', result[1])
print('Row validation OK')
"`
  </verify>
  <done>Row validation logic correctly parses dates, times, numbers, validates event types, and returns per-row validation results with specific error messages.</done>
</task>

<task type="auto">
  <name>Task 2: Add mapping validation endpoint</name>
  <files>
    server/src/routers/imports.py
  </files>
  <action>
  Add `POST /cases/{case_id}/imports/validate` endpoint to imports router:

  1. Params: case_id (path), body (ColumnMapping from schemas)
  2. Dependencies: db (get_db), current_user (get_current_user)
  3. Verify case exists.

  4. Validate the mapping request:
     - Check that session_id exists in _import_sessions. Return 404 if not.
     - Check that session belongs to current user. Return 403 if not.
     - Verify all mapped event fields are in VALID_EVENT_FIELDS. Return 422 for unknown fields.
     - Verify all mapped spreadsheet columns exist in the stored headers. Return 422 for unknown columns.
     - Verify event_date is mapped (required). Return 422 if missing.

  5. Add session_id field to ColumnMapping schema (in import_.py) so it can be submitted with the mapping.

  6. Iterate all stored data rows:
     - Call validate_and_transform_row for each row
     - Build ImportValidationRow for each (row_number 1-indexed, matching spreadsheet row)
     - Count valid and error rows

  7. Store validated results in session: _import_sessions[session_id]["validated_rows"] = list of (is_valid, transformed_data) tuples, and store the mappings.

  8. Return ImportValidationResponse with total_rows, valid_count, error_count, and all rows.
  </action>
  <verify>
  Run: `cd /Users/shiro/projec1/server && python -c "from src.routers.imports import router; routes = [r.path for r in router.routes]; print('Routes:', routes); assert any('validate' in r for r in routes); print('Validate endpoint OK')"`
  </verify>
  <done>Mapping validation endpoint accepts column mappings, validates all rows, returns detailed per-row validation results, and stores validated data for the confirm step.</done>
</task>

</tasks>

<verification>
1. Row validation correctly parses date strings in multiple formats
2. Row validation correctly identifies invalid/missing required fields
3. Mapping endpoint rejects unknown event fields and unmapped event_date
4. Validation response includes per-row error details
5. Validated data is stored in session for confirm step
</verification>

<success_criteria>
- Column mapping accepts user-defined field mappings
- Each row is validated with specific error messages
- event_date is required; other fields are optional
- Validation summary shows counts and per-row details
- Validated data persists in session for batch creation
</success_criteria>

<output>
After completion, create `.planning/phases/05-data-import/05-02-SUMMARY.md`
</output>
