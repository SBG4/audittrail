---
phase: 05-data-import
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/pyproject.toml
  - server/src/services/__init__.py
  - server/src/services/import_parser.py
  - server/src/schemas/import_.py
  - server/src/routers/imports.py
  - server/src/main.py
autonomous: true

must_haves:
  truths:
    - "Server can accept .xlsx and .csv file uploads"
    - "Uploaded Excel files are parsed correctly with headers and data rows"
    - "Uploaded CSV files are parsed with encoding detection (UTF-8, Latin-1, etc.)"
    - "Parse results include column headers, row count, and preview rows"
    - "Invalid file types are rejected with clear error messages"
  artifacts:
    - path: "server/src/services/import_parser.py"
      provides: "Excel and CSV parsing with encoding detection"
    - path: "server/src/schemas/import_.py"
      provides: "Pydantic schemas for import flow"
    - path: "server/src/routers/imports.py"
      provides: "Upload endpoint returning parsed preview"
  key_links:
    - from: "server/src/routers/imports.py"
      to: "server/src/services/import_parser.py"
      via: "parse_excel() and parse_csv() function calls"
    - from: "server/src/main.py"
      to: "server/src/routers/imports.py"
      via: "app.include_router(imports_router)"
---

<objective>
Backend file parsing service for Excel/CSV uploads with encoding detection.

Purpose: Enable the server to accept spreadsheet uploads, detect encoding, parse headers and data rows, and return structured preview data for the column mapping wizard.
Output: Import parser service, Pydantic schemas, and upload endpoint.
</objective>

<execution_context>
@/Users/shiro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/shiro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-data-import/05-RESEARCH.md
@server/src/routers/events.py
@server/src/schemas/event.py
@server/src/deps.py
@server/src/main.py
@server/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies and create import parser service</name>
  <files>
    server/pyproject.toml
    server/src/services/__init__.py
    server/src/services/import_parser.py
  </files>
  <action>
  1. Add `openpyxl>=3.1` and `chardet>=5.0` to server/pyproject.toml dependencies. Run `cd server && uv lock` to update the lock file.

  2. Create `server/src/services/__init__.py` (empty file to make it a package).

  3. Create `server/src/services/import_parser.py` with:

  - `parse_excel(file_bytes: bytes) -> tuple[list[str], list[list]]`:
    - Use `openpyxl.load_workbook(BytesIO(file_bytes), read_only=True, data_only=True)`
    - Get active worksheet, iterate rows with `values_only=True`
    - First non-empty row = headers. Name empty headers as `Column_N`.
    - Remaining rows = data. Filter out completely empty rows.
    - Handle datetime cells: convert to ISO string. Handle None cells: keep as None.
    - Close workbook after reading.
    - Return (headers, data_rows).

  - `parse_csv(file_bytes: bytes) -> tuple[list[str], list[list]]`:
    - Use `chardet.detect(file_bytes[:10000])` for encoding detection.
    - Fallback to UTF-8 if detection returns None or low confidence (<0.5).
    - Decode bytes to string. Strip BOM (`\ufeff`) if present.
    - Use `csv.Sniffer().sniff(text[:5000])` for dialect detection (delimiter, quoting).
    - Wrap in try/except: if Sniffer fails, default to `csv.excel` dialect.
    - Read all rows via `csv.reader`. First row = headers. Rest = data.
    - Filter out empty rows (all cells empty or whitespace).
    - Return (headers, data_rows).

  - `parse_file(filename: str, file_bytes: bytes) -> tuple[list[str], list[list]]`:
    - Determine type from extension (.csv, .xlsx, .xls).
    - Route to parse_csv or parse_excel.
    - Raise ValueError for unsupported types.

  - `normalize_cell_value(value) -> str | int | float | None`:
    - Convert datetime/date/time objects to ISO strings.
    - Convert numbers to int if they have no decimal part.
    - Keep strings as-is, None as None.
  </action>
  <verify>
  Run: `cd /Users/shiro/projec1/server && python -c "from src.services.import_parser import parse_file; print('Import parser OK')"`
  </verify>
  <done>Import parser service exists with parse_excel, parse_csv, and parse_file functions that handle encoding detection and cell normalization.</done>
</task>

<task type="auto">
  <name>Task 2: Create import schemas and upload endpoint</name>
  <files>
    server/src/schemas/import_.py
    server/src/routers/imports.py
    server/src/main.py
  </files>
  <action>
  1. Create `server/src/schemas/import_.py` with Pydantic models:

  - `ImportUploadResponse(BaseModel)`:
    - session_id: str
    - filename: str
    - headers: list[str]
    - row_count: int
    - preview_rows: list[list] (first 10 rows, cells as str|int|float|None)

  - `ColumnMapping(BaseModel)`:
    - mappings: dict[str, str] (spreadsheet column name -> event field name)
    - Event fields allowed: event_type, event_date, event_time, file_name, file_count, file_description, file_type

  - `ImportValidationRow(BaseModel)`:
    - row_number: int
    - valid: bool
    - errors: list[str]
    - data: dict[str, Any]

  - `ImportValidationResponse(BaseModel)`:
    - session_id: str
    - total_rows: int
    - valid_count: int
    - error_count: int
    - rows: list[ImportValidationRow]

  - `ImportConfirmRequest(BaseModel)`:
    - session_id: str

  - `ImportConfirmResponse(BaseModel)`:
    - created_count: int
    - error_count: int
    - errors: list[str]

  2. Create `server/src/routers/imports.py`:

  - Router prefix: `/cases/{case_id}/imports`, tags: `["imports"]`
  - Module-level dict for session storage: `_import_sessions: dict[str, dict] = {}`
  - Import `_verify_case_exists` pattern from events router (copy the helper or import from a shared location -- simplest: duplicate the pattern).

  - `POST /upload` endpoint:
    - Params: case_id (path), file (UploadFile)
    - Dependencies: db (get_db), current_user (get_current_user)
    - Verify case exists.
    - Validate filename extension (.csv, .xlsx). Reject others with 400.
    - Read file contents. Reject if >10MB with 400.
    - Call `parse_file(filename, contents)`.
    - Normalize all cell values via normalize_cell_value.
    - Generate session_id (uuid4).
    - Store in _import_sessions: { session_id: { case_id, headers, data, user_id } }
    - Return ImportUploadResponse with first 10 rows as preview.

  3. Register router in `server/src/main.py`:
  - Import: `from src.routers.imports import router as imports_router`
  - Add: `app.include_router(imports_router)`
  </action>
  <verify>
  Run: `cd /Users/shiro/projec1/server && python -c "from src.routers.imports import router; print('Imports router OK, routes:', [r.path for r in router.routes])"`
  </verify>
  <done>Upload endpoint accepts Excel/CSV files, parses them, returns preview data with headers and sample rows, and stores parsed data in session for subsequent mapping step.</done>
</task>

</tasks>

<verification>
1. Import parser module loads without errors
2. Router registers with FastAPI app
3. Upload endpoint path exists at /cases/{case_id}/imports/upload
4. Schemas validate correctly with sample data
</verification>

<success_criteria>
- openpyxl and chardet added to dependencies
- Import parser handles both .xlsx and .csv with encoding detection
- Upload endpoint accepts files, validates type/size, returns structured preview
- Session storage retains parsed data for subsequent mapping step
</success_criteria>

<output>
After completion, create `.planning/phases/05-data-import/05-01-SUMMARY.md`
</output>
