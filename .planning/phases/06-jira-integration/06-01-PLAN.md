---
phase: 06-jira-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/src/services/__init__.py
  - server/src/services/jira_scraper.py
  - server/src/schemas/jira.py
  - server/src/routers/jira.py
  - server/src/main.py
  - server/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "POST /api/jira/scrape accepts a Jira URL and returns scraped field data"
    - "Scraper uses Playwright headless Chromium to load Jira pages"
    - "Scraper has configurable timeout and error handling"
  artifacts:
    - path: "server/src/services/jira_scraper.py"
      provides: "JiraScraper class with async scrape_issue method"
    - path: "server/src/schemas/jira.py"
      provides: "JiraScrapeRequest and JiraScrapeResponse Pydantic models"
    - path: "server/src/routers/jira.py"
      provides: "POST /jira/scrape endpoint"
  key_links:
    - from: "server/src/routers/jira.py"
      to: "server/src/services/jira_scraper.py"
      via: "JiraScraper instantiation in endpoint"
      pattern: "JiraScraper.*scrape_issue"
    - from: "server/src/main.py"
      to: "server/src/routers/jira.py"
      via: "app.include_router"
      pattern: "include_router.*jira"
---

<objective>
Create the headless browser scraping service using Playwright that can fetch a Jira issue page and return raw scraped field data.

Purpose: Provides the core infrastructure for JIRA-01 and JIRA-02 -- the ability to fetch and parse data from an internal Jira instance via headless browser.
Output: JiraScraper service, Pydantic schemas, and /jira/scrape API endpoint.
</objective>

<execution_context>
@/Users/shiro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/shiro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@server/src/main.py
@server/src/deps.py
@server/src/config.py
@server/pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Jira scraper service and Pydantic schemas</name>
  <files>
    server/src/services/__init__.py
    server/src/services/jira_scraper.py
    server/src/schemas/jira.py
    server/pyproject.toml
  </files>
  <action>
  1. Create `server/src/services/__init__.py` (empty).

  2. Add dependencies to `server/pyproject.toml`:
     - `playwright>=1.49.0`
     - `beautifulsoup4>=4.12.0`
     - `lxml>=5.0.0`

  3. Create `server/src/schemas/jira.py` with:
     - `JiraScrapeRequest(BaseModel)`: `url: str` (the Jira issue URL), `timeout_ms: int = 15000`
     - `JiraScrapeResponse(BaseModel)`: `url: str`, `fields: dict[str, str]` (scraped field name -> value), `raw_fields: dict[str, str]` (all fields found before mapping), `error: str | None = None`, `success: bool`
     - `JiraFieldMappingRead(BaseModel)`: `id: uuid.UUID`, `audit_type_id: uuid.UUID`, `jira_field_name: str`, `case_metadata_key: str` with `model_config = ConfigDict(from_attributes=True)`
     - `JiraFieldMappingCreate(BaseModel)`: `jira_field_name: str`, `case_metadata_key: str`
     - `JiraFieldMappingBulkUpdate(BaseModel)`: `mappings: list[JiraFieldMappingCreate]`

  4. Create `server/src/services/jira_scraper.py` with class `JiraScraper`:
     - `async def scrape_issue(self, url: str, timeout_ms: int = 15000) -> dict[str, str]`:
       - Validate URL starts with "http://" or "https://"
       - Use `async_playwright()` context manager
       - Launch chromium headless with args: `--no-sandbox`, `--disable-setuid-sandbox`, `--disable-dev-shm-usage`
       - Create new page, navigate to URL with `wait_until="networkidle"`, `timeout=timeout_ms`
       - Get `page.content()` HTML
       - Call `self._parse_fields(html)` to extract fields
       - Close browser in finally block
       - Return dict of field_name -> field_value strings
       - On TimeoutError: raise HTTPException 504 "Jira page load timed out"
       - On PlaywrightError: raise HTTPException 502 "Failed to fetch Jira page"

     - `def _parse_fields(self, html: str) -> dict[str, str]`:
       - Use BeautifulSoup with "lxml" parser
       - Extract fields using multiple strategies (for Jira Server and Cloud):

         Strategy A - Jira Server/DC detail view:
         - Look for `#details-module .item` or `.details-layout .item` elements
         - Within each, find label in `.name` or `strong` and value in `.value` or adjacent element
         - Also check `.field-group .field-name` / `.field-value` patterns

         Strategy B - Jira Cloud:
         - Look for `[data-testid*="issue-field"]` elements
         - Extract label from nested label element, value from value element

         Strategy C - Common fields with known selectors:
         - Summary: `#summary-val`, `h1[data-testid*="summary"]`, `#summary_header_id`
         - Status: `#status-val`, `[data-testid*="status"]`
         - Assignee: `#assignee-val`, `[data-testid*="assignee"]`
         - Reporter: `#reporter-val`, `[data-testid*="reporter"]`
         - Priority: `#priority-val`, `[data-testid*="priority"]`
         - Type: `#type-val`, `[data-testid*="issuetype"]`
         - Created: `#created-val`, `[data-testid*="created"]`, `.date-created`
         - Description: `#description-val`, `[data-testid*="description"]`

       - Strip whitespace from all values
       - Skip entries where value is empty
       - Return dict of all extracted fields
  </action>
  <verify>
  - `python -c "from src.services.jira_scraper import JiraScraper; print('OK')"` from server dir
  - `python -c "from src.schemas.jira import JiraScrapeRequest, JiraScrapeResponse; print('OK')"` from server dir
  - Check pyproject.toml includes playwright, beautifulsoup4, lxml
  </verify>
  <done>JiraScraper class exists with scrape_issue() and _parse_fields() methods. Pydantic schemas define request/response models. Dependencies added to pyproject.toml.</done>
</task>

<task type="auto">
  <name>Task 2: Jira scrape API endpoint and router registration</name>
  <files>
    server/src/routers/jira.py
    server/src/main.py
  </files>
  <action>
  1. Create `server/src/routers/jira.py`:
     - `router = APIRouter(prefix="/jira", tags=["jira"])`
     - `POST /scrape` endpoint:
       - Accepts `JiraScrapeRequest` body
       - Requires `current_user` dependency (authenticated)
       - Instantiates `JiraScraper()`
       - Calls `scraper.scrape_issue(body.url, body.timeout_ms)`
       - Returns `JiraScrapeResponse(url=body.url, fields=scraped_fields, raw_fields=scraped_fields, success=True)`
       - On HTTPException: re-raise
       - On any other Exception: return `JiraScrapeResponse(url=body.url, fields={}, raw_fields={}, success=False, error=str(e))`

  2. Update `server/src/main.py`:
     - Add `from src.routers.jira import router as jira_router`
     - Add `app.include_router(jira_router)`
  </action>
  <verify>
  - `python -c "from src.routers.jira import router; print(router.prefix)"` prints "/jira"
  - `grep "jira_router" server/src/main.py` finds the import and include
  </verify>
  <done>POST /api/jira/scrape endpoint exists, is authenticated, delegates to JiraScraper, and returns structured response. Router registered in main.py.</done>
</task>

</tasks>

<verification>
- All imports resolve without errors
- JiraScraper class has both async scrape_issue() and _parse_fields() methods
- Router is registered in main.py FastAPI app
- Pydantic schemas validate correctly
</verification>

<success_criteria>
- POST /api/jira/scrape endpoint accepts a URL and would scrape it via Playwright
- Error handling covers timeout, connection failure, and general errors
- Service layer is cleanly separated from router layer
</success_criteria>

<output>
After completion, create `.planning/phases/06-jira-integration/06-01-SUMMARY.md`
</output>
